{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This part set up database with differents tables. It load data(the scale of data have to do manually and place into data folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES to set up\n",
    "SF = 1 # SETUP the name of the file where result will be write\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import glob\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"TPC-DS Data Loading\").config(\"spark.sql.catalogImplementation\", \"hive\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create all tables\n",
    "with open(os.getcwd() + \"/ddl/Create_database.sql\", 'r') as file: # path/to/tpcds.sql\n",
    "    db_string = file.read()\n",
    "\n",
    "db_string = str.split(db_string, \";\")\n",
    "db_string = db_string[:-1]\n",
    "\n",
    "for string in db_string:\n",
    "    if string != '\\n' or string != '':\n",
    "        spark.sql(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datas into tables\n",
    "data_file = glob.glob(os.getcwd() + \"/data/*\")\n",
    "for i in data_file:\n",
    "    table_name = i.split(\"/\")[-1].split(\".\")[0]\n",
    "    if table_name != \"dbgen_version\":\n",
    "        sql_command = \"select * from \" + table_name\n",
    "        tmp=spark.sql(sql_command)\n",
    "        #print(tmp)\n",
    "        df = spark.read.schema(tmp.schema).csv( i, sep='|')\n",
    "        df.write.mode(\"append\").insertInto(table_name)\n",
    "        print(\"Data from table \" + table_name + \" is inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run queries\n",
    "queries_files = glob.glob(os.getcwd() + \"/Queries/Q*\")\n",
    "print(queries_files)\n",
    "for i in queries_files:\n",
    "    with open( i , 'r') as file:\n",
    "        db_string = file.read()\n",
    "    db_string = str.split(db_string, \";\")[:-1]\n",
    "    #start time\n",
    "    i = 0\n",
    "    for j in db_string:\n",
    "        try:\n",
    "            i+=1\n",
    "            spark.sql(j)\n",
    "        except:\n",
    "            print( str(i) + \" doesn't work\")\n",
    "    #end time\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6  yes\n",
      "72  yes\n",
      "69  yes\n",
      "95  yes\n",
      "96  yes\n",
      "70  yes\n",
      "5  yes\n"
     ]
    }
   ],
   "source": [
    "#run queries\n",
    "\n",
    "SF = 1 # SETUP the scale of data !!!\n",
    "queries_files = glob.glob(os.getcwd() + \"/Queries/consulta*\")\n",
    "j = 0\n",
    "time1=[-1 for i in range(103)]\n",
    "time2=[-1 for i in range(103)]\n",
    "time3=[-1 for i in range(103)]\n",
    "time4=[-1 for i in range(103)]\n",
    "time5=[-1 for i in range(103)]\n",
    "\n",
    "for i in queries_files:\n",
    "    with open( i , 'r') as file:\n",
    "        db_string = file.read()\n",
    "    #start time\n",
    "    number_queries = int(i.split(\"consulta\")[-1].split(\".\")[0])\n",
    "    try:\n",
    "        j+=1\n",
    "        exec_start = time.time()\n",
    "        spark.sql(db_string)\n",
    "        exec_end = time.time()\n",
    "        time1[number_queries-1] = exec_end- exec_start\n",
    "        exec_start = time.time()\n",
    "        spark.sql(db_string)\n",
    "        exec_end = time.time()\n",
    "        time2[number_queries-1] = exec_end- exec_start\n",
    "        exec_start = time.time()\n",
    "        spark.sql(db_string)\n",
    "        exec_end = time.time()\n",
    "        time3[number_queries-1] = exec_end- exec_start\n",
    "        exec_start = time.time()\n",
    "        spark.sql(db_string)\n",
    "        exec_end = time.time()\n",
    "        time4[number_queries-1] = exec_end- exec_start\n",
    "        exec_start = time.time()\n",
    "        spark.sql(db_string)\n",
    "        exec_end = time.time()\n",
    "        time5[number_queries-1] = exec_end- exec_start\n",
    "    except:\n",
    "        print(str(number_queries), \" yes\")\n",
    "\n",
    "f = open(\"result/sf\"+ str(SF) + \".csv\", \"w\")\n",
    "f.write(str(time1)+ '\\n' + str(time2)+ '\\n'+ str(time3)+ '\\n'+ str(time4)+ '\\n' + str(time5))\n",
    "f.close()\n",
    "    \n",
    "    #end time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "6\n",
      "30\n",
      "45\n",
      "1\n",
      "37\n",
      "15\n",
      "35\n",
      "13\n",
      "7\n",
      "36\n",
      "43\n",
      "20\n",
      "66\n",
      "88\n",
      "86\n",
      "21\n",
      "17\n",
      "73\n",
      "98\n",
      "65\n",
      "63\n",
      "41\n",
      "12\n",
      "31\n",
      "57\n",
      "60\n",
      "72\n",
      "44\n",
      "10\n",
      "67\n",
      "83\n",
      "90\n",
      "26\n",
      "76\n",
      "79\n",
      "2\n",
      "99\n",
      "29\n",
      "93\n",
      "87\n",
      "53\n",
      "59\n",
      "71\n",
      "18\n",
      "48\n",
      "49\n",
      "28\n",
      "22\n",
      "89\n",
      "23\n",
      "64\n",
      "33\n",
      "9\n",
      "62\n",
      "16\n",
      "94\n",
      "51\n",
      "69\n",
      "85\n",
      "8\n",
      "101\n",
      "77\n",
      "95\n",
      "39\n",
      "102\n",
      "56\n",
      "61\n",
      "75\n",
      "11\n",
      "81\n",
      "42\n",
      "96\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "55\n",
      "50\n",
      "103\n",
      "100\n",
      "47\n",
      "3\n",
      "68\n",
      "52\n",
      "32\n",
      "58\n",
      "27\n",
      "24\n",
      "97\n",
      "38\n",
      "4\n",
      "14\n",
      "34\n",
      "54\n",
      "25\n",
      "70\n",
      "19\n",
      "5\n",
      "46\n",
      "74\n",
      "40\n",
      "92\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "queries_files = glob.glob(os.getcwd() + \"/Queries/consulta*\")\n",
    "for i in queries_files:\n",
    "    print(i.split(\"consulta\")[-1].split(\".\")[0])\n",
    "\n",
    "#for i in range(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
