{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This part set up database with differents tables. It load data(the scale of data have to do manually and place into data folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES to set up\n",
    "# SETUP the name of the file where result will be write\n",
    "SF = 10 #choose between 1,3,5,8,10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import glob\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"TPC-DS Data Loading\").config(\"spark.sql.catalogImplementation\", \"hive\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create all tables\n",
    "with open(os.getcwd() + \"/ddl/Create_database.sql\", 'r') as file: # path/to/tpcds.sql\n",
    "    db_string = file.read()\n",
    "\n",
    "db_string = str.split(db_string, \";\")\n",
    "db_string = db_string[:-1]\n",
    "\n",
    "for string in db_string:\n",
    "    if string != '\\n' or string != '':\n",
    "        spark.sql(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datas into tables\n",
    "table_loading =[]\n",
    "data_file = glob.glob(os.getcwd() + \"/data/data\"+ str(SF)+\"/*\")\n",
    "for i in data_file:\n",
    "    table_name = i.split(\"/\")[-1].split(\".\")[0]\n",
    "    if table_name != \"dbgen_version\":\n",
    "        sql_command = \"select * from \" + table_name\n",
    "        tmp=spark.sql(sql_command)\n",
    "        #print(tmp)\n",
    "        table_load_start = time.time()\n",
    "        df = spark.read.schema(tmp.schema).csv( i, sep='|')\n",
    "        df.write.mode(\"append\").insertInto(table_name)\n",
    "        table_load_end = time.time()\n",
    "        table_loading.append([table_name, table_load_end - table_load_start])\n",
    "        print(\"Data from table \" + table_name + \" is inserted\")\n",
    "        \n",
    "        \n",
    "#write result        \n",
    "f = open(\"result/table_time_SF\"+ str(SF) + \".csv\", \"w\")\n",
    "f.write( str(table_loading) +'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run queries\n",
    "queries_files = glob.glob(os.getcwd() + \"/Queries/query*\")\n",
    "j = 0\n",
    "time1=[0 for i in range(99)]\n",
    "time2=[0 for i in range(99)]\n",
    "time3=[0 for i in range(99)]\n",
    "time4=[0 for i in range(99)]\n",
    "time5=[0 for i in range(99)]\n",
    "\n",
    "for i in queries_files:\n",
    "    with open( i , 'r') as file:\n",
    "        db_string = file.read()\n",
    "    db_string=db_string.split(\";\")[:-1]\n",
    "    #start time\n",
    "    number_queries = int(i.split(\"query\")[-1].split(\".\")[0])\n",
    "    for i in db_string:\n",
    "        try:\n",
    "            j+=1\n",
    "            exec_start = time.time()\n",
    "            spark.sql(i)\n",
    "            exec_end = time.time()\n",
    "            time1[number_queries-1] += exec_end- exec_start\n",
    "            exec_start = time.time()\n",
    "            spark.sql(i)\n",
    "            exec_end = time.time()\n",
    "            time2[number_queries-1] += exec_end- exec_start\n",
    "            exec_start = time.time()\n",
    "            spark.sql(i)\n",
    "            exec_end = time.time()\n",
    "            time3[number_queries-1] += exec_end- exec_start\n",
    "            exec_start = time.time()\n",
    "            spark.sql(i)\n",
    "            exec_end = time.time()\n",
    "            time4[number_queries-1] += exec_end- exec_start\n",
    "            exec_start = time.time()\n",
    "            spark.sql(i)\n",
    "            exec_end = time.time()\n",
    "            time5[number_queries-1] += exec_end- exec_start\n",
    "            \n",
    "        except:\n",
    "            print(str(number_queries), \" yes\")\n",
    "\n",
    "\n",
    "    \n",
    "    #end time\n",
    "f = open(\"result/sf\"+ str(SF) + \".csv\", \"w\")\n",
    "f.write(str(time1)+ '\\n' + str(time2)+ '\\n'+ str(time3)+ '\\n'+ str(time4)+ '\\n' + str(time5))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.831288814544678, 5.224609613418579, 5.151582717895508, 5.2003748416900635, 5.193056583404541, 5.185740947723389, 5.209424257278442, 5.155352592468262]\n"
     ]
    }
   ],
   "source": [
    "#Throughput TEST 1\n",
    "time_trought=[0,0,0,0,0,0,0,0]\n",
    "\n",
    "for i in range(1,5):\n",
    "    queries_files = glob.glob(os.getcwd() + \"/Query_1-4/query*_stream\"+ str(i)+\"*\")\n",
    "    \n",
    "    for j in queries_files:\n",
    "        with open( j , 'r') as file:\n",
    "            db_string = file.read()\n",
    "            \n",
    "            \n",
    "        db_string=db_string.split(\";\")[:-1]\n",
    "        for k in db_string:\n",
    "            exec_start = time.time()\n",
    "            spark.sql(k)\n",
    "            exec_end = time.time()\n",
    "            time_trought[i-1] += exec_end- exec_start\n",
    "            \n",
    "#some maintenance to do...            \n",
    "            \n",
    "# ???\n",
    "    \n",
    "    \n",
    "#some new THrought test 2\n",
    "for i in range(5,9):\n",
    "    queries_files = glob.glob(os.getcwd() + \"/Query_5-8/query*_stream\"+ str(i)+\"*\")\n",
    "    \n",
    "    for j in queries_files:\n",
    "        with open( j , 'r') as file:\n",
    "            db_string = file.read()\n",
    "            \n",
    "            \n",
    "        db_string=db_string.split(\";\")[:-1]\n",
    "        for k in db_string:\n",
    "            exec_start = time.time()\n",
    "            spark.sql(k)\n",
    "            exec_end = time.time()\n",
    "            time_trought[i-1] += exec_end- exec_start\n",
    "print(time_trought)\n",
    "\n",
    "f = open(\"result/ThroughTestsf\"+ str(SF) + \".csv\", \"w\")\n",
    "f.write(str(time_trought))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#maintenace type 2\n",
    "mt2=[[\"DF_SS\",\"store_sales\",\"store_returns\"],[\"DF_CS\", \"catalog_sales\",\"catalog_returns\"],[\"DF_WS\", \"web_sales\",\"web_returns\"]]\n",
    "for i in mt2:\n",
    "    query1 = \"Delete rows from \" +i[]+ \"with corresponding rows in\" + S +\"where d_date between\"+ Date1 +\"and\"+ Date2\n",
    "    query2 = \"Delete rows from\"+ S +\"where d_date between\" + Date1 +\"and\"+ Date2\n",
    "#mainteance type 3\n",
    "[\"DF_I\", \"Inventory\"]\n",
    "\n",
    "\"Delete rows from Inventory where d_date between \" + Date1 + \" and \"+ Date2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
