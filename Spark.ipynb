{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This part set up database with differents tables. It load data(the scale of data have to do manually and place into data folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES to set up\n",
    "# SETUP the name of the file where result will be write\n",
    "SF = 10 #choose between 1,3,5,8,10\n",
    "maintenance_table_type1= [ \n",
    "                            #\"catalog_sales\", \n",
    "                            \"catalog_returns\",\n",
    "                          \"inventory\",\"store_returns\",\n",
    "                          #\"store_sales\",\n",
    "                            \"web_returns\",\n",
    "                          #\"web_sales\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/28 13:54:59 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.178.59 instead (on interface eno1)\n",
      "23/10/28 13:54:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/28 13:55:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import glob\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"TPC-DS Data Loading\").config(\"spark.sql.catalogImplementation\", \"hive\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/28 13:55:04 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/10/28 13:55:04 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/10/28 13:55:26 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/10/28 13:55:26 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore coucou@127.0.1.1\n",
      "23/10/28 13:55:26 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "23/10/28 13:55:29 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:29 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/10/28 13:55:29 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/10/28 13:55:29 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/10/28 13:55:29 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/10/28 13:55:29 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/call_center specified for non-external table:call_center\n",
      "23/10/28 13:55:30 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:30 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/catalog_page specified for non-external table:catalog_page\n",
      "23/10/28 13:55:30 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:30 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/catalog_returns specified for non-external table:catalog_returns\n",
      "23/10/28 13:55:30 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:30 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/catalog_sales specified for non-external table:catalog_sales\n",
      "23/10/28 13:55:30 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:30 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/customer specified for non-external table:customer\n",
      "23/10/28 13:55:31 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:31 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/customer_address specified for non-external table:customer_address\n",
      "23/10/28 13:55:31 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:31 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/customer_demographics specified for non-external table:customer_demographics\n",
      "23/10/28 13:55:31 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:31 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/date_dim specified for non-external table:date_dim\n",
      "23/10/28 13:55:31 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:31 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/household_demographics specified for non-external table:household_demographics\n",
      "23/10/28 13:55:31 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:31 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/income_band specified for non-external table:income_band\n",
      "23/10/28 13:55:31 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:31 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/inventory specified for non-external table:inventory\n",
      "23/10/28 13:55:31 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:31 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/item specified for non-external table:item\n",
      "23/10/28 13:55:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:32 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/promotion specified for non-external table:promotion\n",
      "23/10/28 13:55:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:32 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/reason specified for non-external table:reason\n",
      "23/10/28 13:55:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:32 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/ship_mode specified for non-external table:ship_mode\n",
      "23/10/28 13:55:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:32 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/store specified for non-external table:store\n",
      "23/10/28 13:55:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:32 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/store_returns specified for non-external table:store_returns\n",
      "23/10/28 13:55:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:32 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/store_sales specified for non-external table:store_sales\n",
      "23/10/28 13:55:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:32 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/time_dim specified for non-external table:time_dim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/28 13:55:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:32 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/warehouse specified for non-external table:warehouse\n",
      "23/10/28 13:55:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:32 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/web_page specified for non-external table:web_page\n",
      "23/10/28 13:55:33 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:33 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/web_returns specified for non-external table:web_returns\n",
      "23/10/28 13:55:33 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:33 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/web_sales specified for non-external table:web_sales\n",
      "23/10/28 13:55:33 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/10/28 13:55:33 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DS/spark-warehouse/web_site specified for non-external table:web_site\n"
     ]
    }
   ],
   "source": [
    "#create all tables\n",
    "with open(os.getcwd() + \"/ddl/Create_database.sql\", 'r') as file: # path/to/tpcds.sql\n",
    "    db_string = file.read()\n",
    "\n",
    "db_string = str.split(db_string, \";\")\n",
    "db_string = db_string[:-1]\n",
    "\n",
    "for string in db_string:\n",
    "    if string != '\\n' or string != '':\n",
    "        spark.sql(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/28 13:55:43 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table customer is inserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table customer_address is inserted\n",
      "Data from table promotion is inserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/28 13:55:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table date_dim is inserted\n",
      "Data from table ship_mode is inserted\n",
      "Data from table store is inserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table time_dim is inserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table web_returns is inserted\n",
      "Data from table reason is inserted\n",
      "Data from table call_center is inserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table store_sales is inserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table item is inserted\n",
      "Data from table catalog_page is inserted\n",
      "Data from table warehouse is inserted\n",
      "Data from table web_page is inserted\n",
      "Data from table web_site is inserted\n",
      "Data from table household_demographics is inserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table catalog_sales is inserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table customer_demographics is inserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table store_returns is inserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table web_sales is inserted\n",
      "Data from table income_band is inserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table catalog_returns is inserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from table inventory is inserted\n"
     ]
    }
   ],
   "source": [
    "#load datas into tables\n",
    "table_loading =[]\n",
    "data_file = glob.glob(os.getcwd() + \"/data/data\"+ str(SF)+\"/*\")\n",
    "for i in data_file:\n",
    "    table_name = i.split(\"/\")[-1].split(\".\")[0]\n",
    "    if table_name != \"dbgen_version\":\n",
    "        sql_command = \"select * from \" + table_name\n",
    "        tmp=spark.sql(sql_command)\n",
    "        table_load_start = time.time()\n",
    "        df = spark.read.schema(tmp.schema).csv( i, sep='|')\n",
    "        df.write.mode(\"append\").insertInto(table_name)\n",
    "        table_load_end = time.time()\n",
    "        table_loading.append([table_name, table_load_end - table_load_start])\n",
    "        print(\"Data from table \" + table_name + \" is inserted\")\n",
    "        \n",
    "        \n",
    "#write result        \n",
    "f = open(\"result/table_time_SF\"+ str(SF) + \".csv\", \"w\")\n",
    "f.write( str(table_loading) +'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run queries\n",
    "queries_files = glob.glob(os.getcwd() + \"/Queries/query*\")\n",
    "time1=[0 for i in range(99)]\n",
    "time2=[0 for i in range(99)]\n",
    "time3=[0 for i in range(99)]\n",
    "time4=[0 for i in range(99)]\n",
    "time5=[0 for i in range(99)]\n",
    "\n",
    "for i in queries_files:\n",
    "    with open( i , 'r') as file:\n",
    "        db_string = file.read()\n",
    "    db_string=db_string.split(\";\")[:-1]\n",
    "    #run same query 5 times\n",
    "    number_queries = int(i.split(\"query\")[-1].split(\".\")[0])\n",
    "    for i in db_string:\n",
    "        exec_start = time.time()\n",
    "        df = spark.sql(i)\n",
    "        exec_end = time.time()\n",
    "        time1[number_queries-1] += exec_end- exec_start\n",
    "        exec_start = time.time()\n",
    "        spark.sql(i)\n",
    "        exec_end = time.time()\n",
    "        time2[number_queries-1] += exec_end- exec_start\n",
    "        exec_start = time.time()\n",
    "        spark.sql(i)\n",
    "        exec_end = time.time()\n",
    "        time3[number_queries-1] += exec_end- exec_start\n",
    "        exec_start = time.time()\n",
    "        spark.sql(i)\n",
    "        exec_end = time.time()\n",
    "        time4[number_queries-1] += exec_end- exec_start\n",
    "        exec_start = time.time()\n",
    "        spark.sql(i)\n",
    "        exec_end = time.time()\n",
    "        time5[number_queries-1] += exec_end- exec_start\n",
    "\n",
    "\n",
    "    \n",
    "    #end time\n",
    "f = open(\"result/sf\"+ str(SF) + \".csv\", \"w\")\n",
    "f.write(str(time1)+ '\\n' + str(time2)+ '\\n'+ str(time3)+ '\\n'+ str(time4)+ '\\n' + str(time5))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Throughput TEST 1\n",
    "time_trought=[0,0,0,0,0,0,0,0]\n",
    "time_maintenance = [0,0]\n",
    "for k in range(2): #boucle on Throughput TEST\n",
    "    startStreamNumber = str(k*4+1)\n",
    "    endStreamNumber = str(k*4+4)\n",
    "    for i in range(k*4+1,k*4+5): #boucle on stream\n",
    "        queries_files = glob.glob(os.getcwd() + \"/Query_\"+ startStreamNumber +\"-\"+ endStreamNumber +\"/query*_stream\"+ str(i)+\"*\")\n",
    "        for j in queries_files:\n",
    "            with open( j , 'r') as file:\n",
    "                db_string = file.read()\n",
    "\n",
    "\n",
    "            db_string=db_string.split(\";\")[:-1]\n",
    "            for m in db_string:\n",
    "                exec_start = time.time()\n",
    "                spark.sql(m)\n",
    "                exec_end = time.time()\n",
    "                time_trought[i-1] += exec_end- exec_start\n",
    "    \n",
    "    #maintenance of type 1 inserting\n",
    "    maintenance_start = time.time()\n",
    "    for i in maintenance_table_type1: #name of table\n",
    "        sql_command = \"select * from \" + str(i)\n",
    "        tmp=spark.sql(sql_command)\n",
    "        for j in range(k*4+1,k*4+5):\n",
    "            df = spark.read.schema(tmp.schema).csv(os.getcwd() + \"/maintenance/maintenance\"+ str(SF)+\"/s_\"+ str(i) +\"_\"+str(j)+\".dat\", sep='|')\n",
    "            #df.write.mode(\"append\").insertInto(str(i))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # methods of type 2#\n",
    "    for j in range(k*4+1,k*4+5):\n",
    "        with open(os.getcwd() + \"/maintenance/maintenance\"+ str(SF)+ \"/delete_\"+ str(j)+\".dat\", 'r') as file:\n",
    "            db_string = file.read()\n",
    "        db_string = db_string.split(\"|\")[:-1]\n",
    "        for l in range(0,len(db_string),2):\n",
    "            method_2_string = \"\"\"\n",
    "                --> DF_CS <--\n",
    "                DELETE cr\n",
    "                FROM catalog_returns AS cr\n",
    "                JOIN catalog_sales AS cs ON cr.cr_order_number = cs.cs_order_number AND cr.cr_item_sk = cs.cs_item_sk\n",
    "                JOIN date_dim AS dd ON cs.cs_sold_date_sk = dd.d_date_sk\n",
    "                WHERE dd.d_date BETWEEN \"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]+\"\"\";\n",
    "\n",
    "                DELETE cs\n",
    "                FROM catalog_sales AS cs\n",
    "                JOIN date_dim AS dd ON cs.cs_sold_date_sk = dd.d_date_sk\n",
    "                WHERE dd.d_date BETWEEN \"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]+\"\"\";\n",
    "\n",
    "                --> DF_SS <--\n",
    "                DELETE sr\n",
    "                FROM store_returns AS sr\n",
    "                JOIN store_sales AS ss ON sr.sr_ticket_number = ss.ss_ticket_number AND sr.sr_item_sk = ss.ss_item_sk\n",
    "                JOIN date_dim AS dd ON ss.ss_sold_date_sk = dd.d_date_sk\n",
    "                WHERE dd.d_date BETWEEN \"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]+\"\"\";\n",
    "\n",
    "                DELETE ss\n",
    "                FROM store_sales AS ss\n",
    "                JOIN date_dim AS dd ON ss.ss_sold_date_sk = dd.d_date_sk\n",
    "                WHERE dd.d_date BETWEEN \"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]+\"\"\";\n",
    "\n",
    "                --> DF_WS <--\n",
    "                DELETE wr\n",
    "                FROM web_returns AS wr\n",
    "                JOIN web_sales AS ws ON wr.wr_order_number = ws.ws_order_number AND wr.wr_item_sk = ws.ws_item_sk\n",
    "                JOIN date_dim AS dd ON ws.ws_sold_date_sk = dd.d_date_sk\n",
    "                WHERE dd.d_date BETWEEN \"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]+\"\"\";\n",
    "\n",
    "                DELETE ws\n",
    "                FROM web_sales AS ws\n",
    "                JOIN date_dim AS dd ON ws.ws_sold_date_sk = dd.d_date_sk\n",
    "                WHERE dd.d_date BETWEEN \"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]+\"\"\";\n",
    "\n",
    "                \"\"\"\n",
    "            method_2_string = method_2_string.split(\";\")[:-1]\n",
    "            for sql_command in method_2_string:\n",
    "                #spark.sql(sql_command)\n",
    "                pass\n",
    "\n",
    "    \n",
    "    #maintenance of type 3 # delete inventory\n",
    "    for j in range(k*4+1,k*4+5):\n",
    "        with open(os.getcwd() + \"/maintenance/maintenance\"+ str(SF)+ \"/inventory_delete_\"+ str(j)+\".dat\", 'r') as file:\n",
    "            db_string = file.read()\n",
    "        db_string = db_string.split(\"|\")[:-1]\n",
    "        for l in range(0,len(db_string),2):\n",
    "            sql_command = \"\"\"DELETE FROM inventory i JOIN date_dim d ON i.inv_date_sk = d.d_date_sk WHERE d.d_date BETWEEN\"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]\n",
    "            #spark.sql(sql_command)\n",
    "    maintenance_end = time.time()\n",
    "    time_maintenance[k] = maintenance_end - maintenance_start\n",
    "f = open(\"result/ThroughTest_and_Maintenance_sf\"+ str(SF) + \".csv\", \"w\")\n",
    "f.write(str(time_trought)+'\\n' + str(time_maintenance))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#methods of type 1\n",
    "maintenance_start = time.time()\n",
    "for i in maintenance_table_type1: #name of table\n",
    "    sql_command = \"select * from \" + str(i)\n",
    "    tmp=spark.sql(sql_command)\n",
    "    for j in range(1,5):     \n",
    "        df = spark.read.schema(tmp.schema).csv(os.getcwd() + \"/maintenance/maintenance\"+ str(SF)+\"/s_\"+ str(i) +\"_\"+str(j)+\".dat\", sep='|')\n",
    "        #df.write.mode(\"append\").insertInto(table_name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# methods of type 2#\n",
    "for j in range(1,5):\n",
    "    with open(os.getcwd() + \"/maintenance/maintenance\"+ str(SF)+ \"/delete_\"+ str(j)+\".dat\", 'r') as file:\n",
    "        db_string = file.read()\n",
    "    db_string = db_string.split(\"|\")[:-1]\n",
    "    for l in range(0,len(db_string),2):\n",
    "        method_2_string = \"\"\"\n",
    "            --> DF_CS <--\n",
    "            DELETE FROM catalog_returns AS cr\n",
    "            JOIN catalog_sales AS cs ON cr.cr_order_number = cs.cs_order_number AND cr.cr_item_sk = cs.cs_item_sk\n",
    "            JOIN date_dim AS dd ON cs.cs_sold_date_sk = dd.d_date_sk\n",
    "            WHERE dd.d_date BETWEEN \"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]+\"\"\";\n",
    "\n",
    "            DELETE FROM catalog_sales AS cs\n",
    "            JOIN date_dim AS dd ON cs.cs_sold_date_sk = dd.d_date_sk\n",
    "            WHERE dd.d_date BETWEEN \"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]+\"\"\";\n",
    "\n",
    "            --> DF_SS <--\n",
    "            DELETE FROM store_returns AS sr\n",
    "            JOIN store_sales AS ss ON sr.sr_ticket_number = ss.ss_ticket_number AND sr.sr_item_sk = ss.ss_item_sk\n",
    "            JOIN date_dim AS dd ON ss.ss_sold_date_sk = dd.d_date_sk\n",
    "            WHERE dd.d_date BETWEEN \"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]+\"\"\";\n",
    "\n",
    "            DELETE FROM store_sales AS ss\n",
    "            JOIN date_dim AS dd ON ss.ss_sold_date_sk = dd.d_date_sk\n",
    "            WHERE dd.d_date BETWEEN \"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]+\"\"\";\n",
    "\n",
    "            --> DF_WS <--\n",
    "            DELETE FROM web_returns AS wr\n",
    "            JOIN web_sales AS ws ON wr.wr_order_number = ws.ws_order_number AND wr.wr_item_sk = ws.ws_item_sk\n",
    "            JOIN date_dim AS dd ON ws.ws_sold_date_sk = dd.d_date_sk\n",
    "            WHERE dd.d_date BETWEEN \"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]+\"\"\";\n",
    "\n",
    "            DELETE FROM web_sales AS ws\n",
    "            JOIN date_dim AS dd ON ws.ws_sold_date_sk = dd.d_date_sk\n",
    "            WHERE dd.d_date BETWEEN \"\"\" + db_string[l] + \"\"\" AND \"\"\" +db_string[l+1]+\"\"\";\n",
    "            \n",
    "            \"\"\"\n",
    "        method_2_string = method_2_string.split(\";\")[:-1]\n",
    "        for sql_command in method_2_string:\n",
    "            spark.sql(sql_command)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# methods of type 3 #\n",
    "\n",
    "for j in range(1,5):\n",
    "    with open(os.getcwd() + \"/maintenance/maintenance\"+ str(SF)+ \"/inventory_delete_\"+ str(j)+\".dat\", 'r') as file:\n",
    "        db_string = file.read()\n",
    "    db_string = db_string.split(\"|\")[:-1]\n",
    "    for l in range(0,len(db_string),2):\n",
    "        sql_command = \"\"\"select i.inv_date_sk from inventory i, date_dim d WHERE i.inv_date_sk = d.d_date_sk and d.d_date not BETWEEN '\"\"\" + db_string[l] + \"\"\"' AND '\"\"\" +db_string[l+1] + \"\"\"'\"\"\"\n",
    "        tmpdf=spark.sql(sql_command)\n",
    "        tmpdf.write.mode(\"overwrite\").insertInto(\"inventory\")\n",
    "        print(\"overite append\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
